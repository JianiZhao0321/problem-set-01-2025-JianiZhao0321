"http","https","www",
"frankfurt","brussels","european","union","eu"
)
poverty_tokens <- poverty_tokens_raw %>%
tokens_tolower() %>%
tokens_remove(extraneous_terms, padding = FALSE)
# 3) n-grams
poverty_tokens_ng <- tokens_ngrams(poverty_tokens, n = 1:2)
# 4) build dfm and trim very rare/common terms
dfm_poverty <- dfm(poverty_tokens_ng)
# 5) trim extremes: appear in <5% of docs or >95% of docs
dfm_poverty_trim <- dfm_trim(
dfm_poverty,
min_docfreq = 0.05,
max_docfreq = 0.95,
docfreq_type = "prop",
verbose = TRUE)
# 6) quick inspection
topfeatures(dfm_poverty_trim, 30)
featnames(dfm_poverty_trim)[1:50]
domain_stopwords <- c(
"euro", "euro_area", "monetary", "policy", "monetary_policy",
"ecb", "central", "bank", "banks", "european", "union"
)
dfm_poverty_trim2 <- dfm_remove(dfm_poverty_trim, pattern = domain_stopwords)
topfeatures(dfm_poverty_trim2, 100)
# 7) Look at a small doc-term snapshot
convert(dfm_poverty_trim2[1:5, 1:10], to = "matrix")
# wordcloud
textplot_wordcloud(
dfm_poverty_trim2,
random_order = FALSE,
rotation = .25,
max_words = 100,
min_size = 0.5,
max_size = 2.8)
poverty_corpus
# see the output ---------------------------------------------------
summary(poverty_corpus,  n = 5)
class(poverty_corpus)
str(poverty_corpus)
# 1) tokenization
# remove the punctuation, numbers, symbols, and URLs
poverty_tokens_raw <- tokens(
poverty_corpus,
remove_punct   = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE,
remove_url     = TRUE
)
# see the output ---------------------------------------------------
summary(poverty_corpus,  n = 5)
class(poverty_corpus)
str(poverty_corpus)
str(poverty_tokens_raw)
# 2) Lowercase + stopwords + custom "extraneous" terms
extraneous_terms <- c(
stopwords("en"),
"speech","slides","member","executive","board","president","vice",
"chart","figure","source","note","notes",
"ecb","eurostat","bce","eib","iea","imf","oecd",
"http","https","www",
"frankfurt","brussels","european","union","eu"
)
poverty_tokens <- poverty_tokens_raw %>%
tokens_tolower() %>%
tokens_remove(extraneous_terms, padding = FALSE)
poverty_tokens_ng
poverty_tokens[[1]]
# 3) n-grams
poverty_tokens_ng <- tokens_ngrams(poverty_tokens, n = 2)
# 4) build dfm and trim very rare/common terms
dfm_poverty <- dfm(poverty_tokens_ng)
poverty_tokens_ng
# 3) n-grams
poverty_tokens_ng <- tokens_ngrams(poverty_tokens, n = 1:2)
# 4) build dfm and trim very rare/common terms
dfm_poverty <- dfm(poverty_tokens_ng)
# 5) trim extremes: appear in <5% of docs or >95% of docs
dfm_poverty_trim <- dfm_trim(
dfm_poverty,
min_docfreq = 0.05,
max_docfreq = 0.95,
docfreq_type = "prop",
verbose = TRUE)
# 5) trim extremes: appear in <5% of docs or >95% of docs
dfm_poverty_trim <- dfm_trim(
dfm_poverty,
min_docfreq = 0.05,
max_docfreq = 0.95,
docfreq_type = "prop",
verbose = TRUE)
# 4) build Documentâ€“Feature Matrix(dfm)
dfm_poverty <- dfm(poverty_tokens_ng)
# 5) trim extremes: appear in <5% of docs or >95% of docs
dfm_poverty_trim <- dfm_trim(
dfm_poverty,
min_docfreq = 0.05,
max_docfreq = 0.95,
docfreq_type = "prop",
verbose = TRUE)
# 6) quick inspection
topfeatures(dfm_poverty_trim, 30)
featnames(dfm_poverty_trim)[1:50]
domain_stopwords <- c(
"euro", "euro_area", "monetary", "policy", "monetary_policy",
"ecb", "central", "bank", "banks", "european", "union"
)
dfm_poverty_trim2 <- dfm_remove(dfm_poverty_trim, pattern = domain_stopwords)
topfeatures(dfm_poverty_trim2, 100)
# 7) Look at a small doc-term snapshot
convert(dfm_poverty_trim2[1:5, 1:10], to = "matrix")
# wordcloud
textplot_wordcloud(
dfm_poverty_trim2,
random_order = FALSE,
rotation = .25,
max_words = 100,
min_size = 0.5,
max_size = 2.8)
# wordcloud
textplot_wordcloud(
dfm_poverty_trim2,
random_order = FALSE,
rotation = .25,
max_words = 100,
min_size = 0.5,
max_size = 2.8)
library(tidyverse)
library(quanteda)
library(preText)
# 7) Look at a small doc-term snapshot
convert(dfm_poverty_trim2[1:5, 1:10], to = "matrix")
# wordcloud
textplot_wordcloud(
dfm_poverty_trim2,
random_order = FALSE,
rotation = .25,
max_words = 100,
min_size = 0.5,
max_size = 2.8)
install.packages("quanteda.textplots")
install.packages("quanteda.textplots")
library(quanteda.textplots)
# wordcloud
library(quanteda.textplots)
textplot_wordcloud(
dfm_poverty_trim2,
random_order = FALSE,
rotation = .25,
max_words = 100,
min_size = 0.5,
max_size = 2.8)
## Question 3-----------------------------------------------------
Using [Danny and Spirling's Pretext](https://github.com/matthewjdenny/preText), tell me which of the pre-processing steps makes a more substantive difference in the data transformation? Would you keep all these steps or would you follow the PreText recommendations? Use a sample of the data to solve this question.
pacman::p_load(quanteda, preText, dplyr)
# Take a smaller sample of speeches for runtime
set.seed(123)
poverty_sample <- poverty_speeches %>%
slice_sample(n = 40)
c
# Run factorial preprocessing (tries different combinations)
preproc_docs <- factorial_preprocessing(
poverty_corpus_sample,
use_ngrams = FALSE,
infrequent_term_threshold = 0.2, # drop rare terms
verbose = TRUE
)
# Take a smaller sample of speeches for runtime
set.seed(123)
poverty_sample <- poverty_speeches %>%
slice_sample(n = 40)
# Make a corpus
poverty_corpus_sample <- corpus(poverty_sample, text_field = "contents")
# Inspect the object
str(preproc_docs, max.level = 1)
# Run preText
pretext_results <- preText(
preproc_docs,
dataset_name = "ECB Poverty Speechesz_sample",
distance_method = "cosine",
num_comparisons = 20,  # how many random comparisons
verbose = TRUE
)
# Make a corpus
poverty_corpus_sample <- corpus(poverty_sample, text_field = "contents")
# Run factorial preprocessing (tries different combinations)
preproc_docs <- factorial_preprocessing(
poverty_corpus_sample,
use_ngrams = FALSE,
infrequent_term_threshold = 0.2, # drop rare terms
verbose = TRUE
)
# Inspect the object
str(preproc_docs, max.level = 1)
# Run factorial preprocessing (tries different combinations)
preproc_docs <- factorial_preprocessing(
poverty_corpus_sample,
use_ngrams = FALSE,
infrequent_term_threshold = 0.2, # drop rare terms
verbose = TRUE
)
# Run preText
pretext_results <- preText(
preproc_docs,
dataset_name = "ECB Poverty Speechesz_sample",
distance_method = "cosine",
num_comparisons = 20,  # how many random comparisons
verbose = TRUE
)
# Plot the scores
preText_score_plot(pretext_results)
# Regression coefficients: which steps matter?
regression_coefficient_plot(pretext_results,
remove_intercept = TRUE)
# Plot the scores
preText_score_plot(pretext_results)
# Regression coefficients: which steps matter?
regression_coefficient_plot(pretext_results,
remove_intercept = TRUE)
# Plot the scores
preText_score_plot(pretext_results)
# Regression coefficients: which steps matter?
regression_coefficient_plot(pretext_results,
remove_intercept = TRUE)
which preprocessing steps matter most?
Would I keep all the steps?
## Question 4 -----------------------------------------------------
Pick an important source of variation in your data (for example: date, author identity, location, etc.). Subset the data along this dimension, and discuss which words discriminate better each group. You can do this by using TF-IDF, PMI, log(share $word_i$ in group a/share $word_i$ in b), or create a measure similar to Ban's article.
# build corpus from your subset
poverty_corpus <- corpus(poverty_speeches, text_field = "contents")
# tokenize (basic preprocessing)
toks <- tokens(poverty_corpus,
remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE) %>%
tokens_tolower() %>%
tokens_remove(stopwords("en"))
# create dfm grouped by speaker
dfm_by_speaker <- toks %>%
dfm() %>%
dfm_group(groups = docvars(poverty_corpus, "speakers"))
dfm_by_speaker
# look at top features per speaker
topfeatures(dfm_tfidf_speaker["Christine Lagarde", ], 20)
dfm_by_speaker
dfm_tfidf_speaker <- dfm_tfidf(dfm_by_speaker, scheme_tf = "prop")
# look at top features per speaker
topfeatures(dfm_tfidf_speaker["Christine Lagarde", ], 20)
topfeatures(dfm_tfidf_speaker["Isabel Schnabel", ], 20)
topfeatures(dfm_tfidf_speaker["Piero Cipollone", ], 20)
## Question 5-----------------------------------------------------
Create a dictionary (or use a pre-existing dictionary) to measure a topic of interest in your data. It can be anything, sentiment, tone, misinformation, any topic of interest. Label your documents, and visualize the prevalence of your classes.
library(quanteda.dictionaries)
data_dictionary_LSD2015
# apply dictionary
dfm_sentiment <- dfm_lookup(dfm_poverty_trim2, dictionary = data_dictionary_LSD2015)
# inspect first rows
head(dfm_sentiment)
# aggregate by year
docvars(poverty_corpus, "year") <- lubridate::year(docvars(poverty_corpus, "date"))
sent_by_year <- convert(dfm_sentiment, to="data.frame") %>%
mutate(year = docvars(poverty_corpus, "year")) %>%
group_by(year) %>%
summarise(across(c(positive, negative), sum))
sent_by_year
sent_by_year_long <- tidyr::pivot_longer(sent_by_year, cols=c(positive, negative),
names_to="sentiment", values_to="count")
ggplot(sent_by_year_long, aes(x=year, y=count, fill=sentiment)) +
geom_col(position="dodge") +
theme_minimal() +
labs(title="Positive vs Negative Tone in ECB Poverty Speeches",
x="Year", y="Word Counts")
## Question 6-----------------------------------------------------
head(ecb_speeches)
# 0) create corpus
poverty_corpus <- corpus(poverty_speeches, text_field = "contents")
# see the output
summary(poverty_corpus,  n = 5)
class(poverty_corpus)
str(poverty_corpus)
# 1) tokenization
# remove the punctuation, numbers, symbols, and URLs
poverty_tokens_raw <- tokens(
poverty_corpus,
remove_punct   = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE,
remove_url     = TRUE
)
str(poverty_tokens_raw)
# 2) Lowercase + stopwords + custom "extraneous" terms
extraneous_terms <- c(
stopwords("en"),
"speech","slides","member","executive","board","president","vice",
"chart","figure","source","note","notes",
"ecb","eurostat","bce","eib","iea","imf","oecd",
"http","https","www",
"frankfurt","brussels","european","union","eu"
)
str(poverty_tokens_raw)
str(poverty_tokens_raw)
poverty_tokens_raw(1)
str(poverty_tokens_raw)
poverty_tokens_raw[1]
poverty_speeches[content]
poverty_speeches$content[1]
poverty_speeches
poverty_speeches$contents[1]
# read the speech file
ecb_speeches <- read_delim(
"~/Dev/PPOL6081/fraccaroli_arel-bundock_blyth_replication_20250523/data/all_ECB_speeches_20240426.csv",
delim = "|")
# 2) Lowercase + stopwords + custom "extraneous" terms
extraneous_terms <- c(
stopwords("en"),
"speech","slides","member","executive","board","president","vice",
"chart","figure","source","note","notes",
"ecb","eurostat","bce","eib","iea","imf","oecd",
"http","https","www",
"frankfurt","brussels","european","union","eu"
)
poverty_tokens <- poverty_tokens_raw %>%
tokens_tolower() %>%
tokens_remove(extraneous_terms, padding = FALSE)
# 3) n-grams
poverty_tokens_ng <- tokens_ngrams(poverty_tokens, n = 1:2)
# 4) build Documentâ€“Feature Matrix(dfm)
dfm_poverty <- dfm(poverty_tokens_ng)
# 5) trim extremes: appear in <5% of docs or >95% of docs
dfm_poverty_trim <- dfm_trim(
dfm_poverty,
min_docfreq = 0.05,
max_docfreq = 0.95,
docfreq_type = "prop",
verbose = TRUE)
# 6) quick inspection
topfeatures(dfm_poverty_trim, 30)
featnames(dfm_poverty_trim)[1:50]
# 6) quick inspection
topfeatures(dfm_poverty_trim, 30)
featnames(dfm_poverty_trim)[1:50]
topfeatures(dfm_poverty_trim, 30)
topfeatures(dfm_poverty_trim, 30)
featnames(dfm_poverty_trim)[1:50]
# 6) quick inspection
topfeatures(dfm_poverty_trim, 100)
featnames(dfm_poverty_trim)[1:50]
# 6) quick inspection
topfeatures(dfm_poverty_trim, 100)
featnames(dfm_poverty_trim)[1:50]
topfeatures(dfm_poverty_trim, 100)
textplot_wordcloud(
dfm_poverty_trim2,
random_order = FALSE,
rotation = .25,
max_words = 100,
min_size = 0.5,
max_size = 2.8)
Using [Danny and Spirling's Pretext](https://github.com/matthewjdenny/preText), tell me which of the pre-processing steps makes a more substantive difference in the data transformation? Would you keep all these steps or would you follow the PreText recommendations? Use a sample of the data to solve this question.
# Take a smaller sample of speeches for runtime
set.seed(123)
poverty_sample <- poverty_speeches %>%
slice_sample(n = 40)
# Make a corpus
poverty_corpus_sample <- corpus(poverty_sample, text_field = "contents")
# Run factorial preprocessing (tries different combinations)
preproc_docs <- factorial_preprocessing(
poverty_corpus_sample,
use_ngrams = FALSE,
infrequent_term_threshold = 0.2, # drop rare terms
verbose = TRUE
)
# Run preText
pretext_results <- preText(
preproc_docs,
dataset_name = "ECB Poverty Speechesz_sample",
distance_method = "cosine",
num_comparisons = 20,  # how many random comparisons
verbose = TRUE
)
# Plot the scores
preText_score_plot(pretext_results)
# Regression coefficients: which steps matter?
regression_coefficient_plot(pretext_results,
remove_intercept = TRUE)
which preprocessing steps matter most?
Would I keep all the steps?
## Question 4 -----------------------------------------------------
Pick an important source of variation in your data (for example: date, author identity, location, etc.). Subset the data along this dimension, and discuss which words discriminate better each group. You can do this by using TF-IDF, PMI, log(share $word_i$ in group a/share $word_i$ in b), or create a measure similar to Ban's article.
# build corpus from your subset
poverty_corpus <- corpus(poverty_speeches, text_field = "contents")
# build corpus from your subset
poverty_corpus <- corpus(poverty_speeches, text_field = "contents")
source("~/Dev/PPOL6081/week2_introduction_to_text_analysis.R")
str(poverty_tokens_raw)
# 7) Look at a small doc-term snapshot
convert(dfm_poverty_trim2[1:5, 1:10], to = "matrix")
# wordcloud
library(quanteda.textplots)
textplot_wordcloud(
dfm_poverty_trim2,
random_order = FALSE,
rotation = .25,
max_words = 100,
min_size = 0.5,
max_size = 2.8)
preproc_docs <- factorial_preprocessing(
poverty_corpus,
use_ngrams = FALSE,
infrequent_term_threshold = 0.2, # drop rare terms
verbose = TRUE)
# Run preText
pretext_results <- preText(
preproc_docs,
dataset_name = "ECB Poverty Speechesz",
distance_method = "cosine",
num_comparisons = 20,  # how many random comparisons
verbose = TRUE
)
# Plot the scores
preText_score_plot(pretext_results)
# Regression coefficients: which steps matter?
regression_coefficient_plot(pretext_results,
remove_intercept = TRUE)
preText_score_plot(pretext_results)
pacman::p_load(quanteda, preText, dplyr)
# Run factorial preprocessing (tries different combinations)
preproc_docs <- factorial_preprocessing(
poverty_corpus,
use_ngrams = FALSE,
infrequent_term_threshold = 0.2, # drop rare terms
verbose = TRUE)
# Run preText
pretext_results <- preText(
preproc_docs,
dataset_name = "ECB Poverty Speeches",
distance_method = "cosine",
num_comparisons = 20,  # how many random comparisons
verbose = TRUE
)
# Plot the scores
preText_score_plot(pretext_results)
# Plot the scores
preText_score_plot(pretext_results)
# Regression coefficients: which steps matter?
regression_coefficient_plot(pretext_results,
remove_intercept = TRUE)
# Plot the scores
preText_score_plot(pretext_results)
# Regression coefficients: which steps matter?
regression_coefficient_plot(pretext_results,
remove_intercept = TRUE)
# create dfm grouped by speaker
dfm_by_speaker <- dfm_poverty_trim2 %>%
dfm() %>%
dfm_group(groups = docvars(poverty_corpus, "speakers"))
dfm_by_speaker
dfm_tfidf_speaker <- dfm_tfidf(dfm_by_speaker, scheme_tf = "prop")
# look at top features per speaker
topfeatures(dfm_tfidf_speaker["Christine Lagarde", ], 20)
topfeatures(dfm_tfidf_speaker["Isabel Schnabel", ], 20)
topfeatures(dfm_tfidf_speaker["Piero Cipollone", ], 20)
dfm_by_speaker
dfm_tfidf_speaker <- dfm_tfidf(dfm_by_speaker, scheme_tf = "prop")
dfm_tfidf_speaker
dfm_tfidf_speaker
# look at top features per speaker
topfeatures(dfm_tfidf_speaker["Christine Lagarde", ], 20)
topfeatures(dfm_tfidf_speaker["Isabel Schnabel", ], 20)
# look at top features per speaker
topfeatures(dfm_tfidf_speaker["Christine Lagarde", ], 20)
topfeatures(dfm_tfidf_speaker["Isabel Schnabel", ], 20)
topfeatures(dfm_tfidf_speaker["Piero Cipollone", ], 20)
sent_by_year
sent_by_y
sent_by_year_long <- tidyr::pivot_longer(sent_by_year, cols=c(positive, negative),
names_to="sentiment", values_to="count")
ggplot(sent_by_year_long, aes(x=year, y=count, fill=sentiment)) +
geom_col(position="dodge") +
theme_minimal() +
labs(title="Positive vs Negative Tone in ECB Poverty Speeches",
x="Year", y="Word Counts")
Using cosine similarity, grab the closest 10 documents to each reference document you read before. Read those. How well does the cosine similarity work? Try with another measure of distance. Anything interesting there?
# Build TF-IDF dfm
dfm_tfidf_speeches <- dfm_tfidf(dfm_poverty, scheme_tf = "prop")
# Build TF-IDF dfm
dfm_tfidf_speeches <- dfm_tfidf(dfm_poverty_trim2, scheme_tf = "prop")
# cosine similarity
simil_speeches <- textstat_simil(dfm_tfidf_speeches,
margin = "documents",
method = "cosine")
sim_matrix <- as.matrix(simil_speeches)
# Add labels (speaker + date to avoid duplicates)
doc_ids <- paste(docvars(poverty_corpus, "speakers"),
docvars(poverty_corpus, "date"),
sep = "_")
rownames(sim_matrix) <- doc_ids
colnames(sim_matrix) <- doc_ids
# Example: Isabel Schnabel cosine neighbors
cosine_neighbors <- sort(sim_matrix[grep("Isabel Schnabel", rownames(sim_matrix))[1], ],
decreasing = TRUE)[2:11]
# --- Euclidean Distance ---
dist_speeches_euc <- textstat_dist(dfm_tfidf_speeches, method = "euclidean")
dist_matrix <- as.matrix(dist_speeches_euc)
rownames(dist_matrix) <- doc_ids
colnames(dist_matrix) <- doc_ids
# Example: Isabel Schnabel Euclidean neighbors
euclidean_neighbors <- sort(dist_matrix[grep("Isabel Schnabel", rownames(dist_matrix))[1], ])[1:10]
# --- Side-by-side comparison ---
cosine_neighbors
euclidean_neighbors
